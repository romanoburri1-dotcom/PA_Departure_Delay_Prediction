{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed5e770",
   "metadata": {},
   "source": [
    "Alle cleanen Datasets importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d90a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "arr = pd.read_csv(\"../data/processed/clean_FZAG_DATA_arrivals.csv\")\n",
    "dep = pd.read_csv(\"../data/processed/clean_FZAG_DATA_departures.csv\")\n",
    "metar = pd.read_csv(\"../data/processed/clean_metar_lszh_2023-2025.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39347de7",
   "metadata": {},
   "source": [
    "Alle Spalten, die eine Zeit als String angegeben haben umwandeln zu Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd6b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr['actual_in_block_time_utc'] = pd.to_datetime(arr['actual_in_block_time_utc'], utc=True)\n",
    "arr['scheduled_in_block_time_utc'] = pd.to_datetime(arr['scheduled_in_block_time_utc'], utc=True)\n",
    "\n",
    "dep['actual_off_block_time_utc'] = pd.to_datetime(dep['actual_off_block_time_utc'], utc=True)\n",
    "dep['scheduled_off_block_time_utc'] = pd.to_datetime(dep['scheduled_off_block_time_utc'], utc=True)\n",
    "\n",
    "metar['time'] = pd.to_datetime(metar['time'], utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666ce06b",
   "metadata": {},
   "source": [
    "30' Zeitintervall für den Zeitraum der Datasets generieren -> manuell gesetzte Grenzen vom 01.01.23, 00:00 UTC - 27.09.25 00:00 UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fa84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zeitgrenzen definieren\n",
    "start = pd.Timestamp(\"2023-01-01 00:00:00\", tz=\"UTC\")\n",
    "end = pd.Timestamp(\"2025-09-27 00:00:00\", tz=\"UTC\")\n",
    "\n",
    "#30' Zeitintervall generieren\n",
    "time_index = pd.date_range(start=start, end=end, freq='30min', inclusive='left') #inclusive left to include start time and exclude end time\n",
    "\n",
    "#Dataframe mit Zeitintervall erstellen\n",
    "time_windows = pd.DataFrame({'start_time': time_index})\n",
    "time_windows['end_time'] = time_windows['start_time'] + pd.Timedelta(minutes=30)\n",
    "\n",
    "'''\n",
    "Zum überprüfen, ob Dataframe korrekt erstellt wurde\n",
    "\n",
    "print(len(time_windows))\n",
    "print(time_windows.head())\n",
    "print(time_windows.tail())\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f73e5c",
   "metadata": {},
   "source": [
    "Feature-Dataframe erstellen mit time_windows als Basis -> Das wird das finale Dataframe, wo alle Features fürs Modell eingebaut sind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700a090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = time_windows.copy()\n",
    "#print(features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b454d",
   "metadata": {},
   "source": [
    "Helper Dataframe erstellen -> Hier können neue Features eingefügt werden, die nur zur Unterstützung da sind und nicht im finalen \"features_df\" landen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccb57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_df = time_windows.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55850df0",
   "metadata": {},
   "source": [
    "Temporale Komponenten extrahieren und zu helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c51470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract from start_time \n",
    "helper_df['month'] = helper_df['start_time'].dt.month - 1 #January=0, December=11\n",
    "helper_df['day_of_week'] = helper_df['start_time'].dt.dayofweek #Monday=0, Sunday=6\n",
    "helper_df['minute_of_day'] = (\n",
    "        helper_df['start_time'].dt.hour * 60 +\n",
    "        helper_df['start_time'].dt.minute\n",
    "    )  # 0–1439\n",
    "#helper_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac528f27",
   "metadata": {},
   "source": [
    "Hilfsfunktion für zyklische Encodierung erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4305148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sin_cos(x,T):\n",
    "    return np.sin(2 * np.pi * x / T), np.cos(2 * np.pi * x / T)\n",
    "\n",
    "\n",
    "'''\n",
    "T = Periodenlänge, z.B. 12 für Monate, 7 für Wochentage, 1440 für Minuten am Tag\n",
    "x = Wert, der umgewandelt werden soll, z.B. Monat 0-11, Wochentag 0-6, Minute 0-1439 (Schematisch: x ist Element von [0, T-1])\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69d8d3d",
   "metadata": {},
   "source": [
    "Temporale Komponenten zyklisch encodieren und zu helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f591278",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_df['month_sin'], helper_df['month_cos'] = sin_cos(helper_df['month'], 12)\n",
    "helper_df['day_of_week_sin'], helper_df['day_of_week_cos'] = sin_cos(helper_df['day_of_week'], 7)\n",
    "helper_df['minute_of_day_sin'], helper_df['minute_of_day_cos'] = sin_cos(helper_df['minute_of_day'], 1440)\n",
    "\n",
    "#helper_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bea73",
   "metadata": {},
   "source": [
    "Zyklisch encodierte temporale Komponenten zum features_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3400ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the cyclic columns\n",
    "cyclic_cols = [\n",
    "    'start_time',\n",
    "    'month_sin', 'month_cos',\n",
    "    'day_of_week_sin', 'day_of_week_cos',\n",
    "    'minute_of_day_sin', 'minute_of_day_cos'\n",
    "]\n",
    "\n",
    "# Merge them into existing features_df\n",
    "features_df = features_df.merge(helper_df[cyclic_cols], on='start_time', how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12b6cab",
   "metadata": {},
   "source": [
    "METAR Komponenten extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1f2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "metar_use = (\n",
    "    metar.rename(columns={\n",
    "        \"time\": \"metar_time_utc\",\n",
    "        \"wind_dir_cleaned\": \"wind_dir_deg\",\n",
    "        \"wind_speed_cleaned\": \"wind_speed_kt\",\n",
    "        \"vis_cleaned\": \"visibility_m\",\n",
    "        \"temp_cleaned\": \"temperature_c\",\n",
    "        \"dewpt_cleaned\": \"dewpoint_c\",\n",
    "        \"press_cleaned\": \"qnh_hpa\",\n",
    "        \"is_wind_variable\": \"is_wind_variable\"\n",
    "    })\n",
    "    # keep only the columns we actually want\n",
    "    [[\"metar_time_utc\", \"wind_dir_deg\", \"wind_speed_kt\",\n",
    "      \"visibility_m\", \"temperature_c\", \"dewpoint_c\",\n",
    "      \"qnh_hpa\", \"is_wind_variable\"]]\n",
    "    .copy()\n",
    ")\n",
    "\n",
    "# ensure is_wind_variable is boolean True/False\n",
    "metar_use[\"is_wind_variable\"] = metar_use[\"is_wind_variable\"].astype(bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afb7638",
   "metadata": {},
   "source": [
    "Timestamps von helper_df und metar_use sortieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4b1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure both are sorted\n",
    "helper_df   = helper_df.sort_values(\"start_time\").copy()\n",
    "metar_use   = metar_use.sort_values(\"metar_time_utc\").copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7b7fef",
   "metadata": {},
   "source": [
    "Winddirection zyklisch encodieren und Condition erstellen, was passiert, wenn Wind VRB ist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2adc95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rad = np.deg2rad(metar_use['wind_dir_deg'])\n",
    "metar_use['wind_dir_sin'] = np.sin(rad)\n",
    "metar_use['wind_dir_cos'] = np.cos(rad)\n",
    "\n",
    "if \"is_wind_variable\" in metar_use.columns:\n",
    "    mask_var = metar_use[\"is_wind_variable\"].astype(bool)\n",
    "    metar_use.loc[mask_var, [\"wind_dir_sin\", \"wind_dir_cos\"]] = np.nan\n",
    "\n",
    "    #Falls Wind variabel ist, setze wind_dir_sin und wind_dir_cos auf NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3237ad00",
   "metadata": {},
   "source": [
    "Visibility vorbereiten, um danach kategorisiert werden zu können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304cb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Cap all values at 10,000 m\n",
    "#    METAR reports \"99999\" for \"10 km or more\", not literally 99999 m.\n",
    "metar_use['visibility_m_clean'] = metar_use['visibility_m'].clip(upper=10000.0)\n",
    "\n",
    "# 2) Create a boolean flag for \"10 km or more\"\n",
    "#    Preserves the info that visibility exceeded the METAR upper limit.\n",
    "metar_use['vis_ge_10km'] = metar_use['visibility_m'] >= 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df0146a",
   "metadata": {},
   "source": [
    "Visibility kategorisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb6a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins   = [0, 500, 1000, 2000, 5000, 8000, 10000]\n",
    "labels = ['≤500m', '500–1000m', '1–2km', '2–5km', '5–8km', '8–10km']\n",
    "metar_use['vis_category'] = pd.cut(\n",
    "    metar_use['visibility_m_clean'],\n",
    "    bins=bins, labels=labels, include_lowest=True, right=True\n",
    ").astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747af00",
   "metadata": {},
   "source": [
    "Spread (Temperatur - Taupunkt) berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e66e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "metar_use['spread_c'] = metar_use['temperature_c'] - metar_use['dewpoint_c']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ffd659",
   "metadata": {},
   "source": [
    "METAR Komponenten zu helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc44a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_bring = [\n",
    "    'metar_time_utc',\n",
    "    'wind_dir_deg', 'wind_speed_kt', 'is_wind_variable',\n",
    "    'wind_dir_sin', 'wind_dir_cos',\n",
    "    'visibility_m_clean', 'vis_ge_10km', 'vis_category',\n",
    "    'temperature_c', 'dewpoint_c', 'spread_c', 'qnh_hpa'\n",
    "]\n",
    "\n",
    "helper_df = pd.merge_asof(\n",
    "    helper_df,\n",
    "    metar_use[cols_to_bring],\n",
    "    left_on='start_time',\n",
    "    right_on='metar_time_utc',\n",
    "    direction='backward'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4256a",
   "metadata": {},
   "source": [
    "METAR Komponenten zu features_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc780a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.merge(\n",
    "    helper_df[[\n",
    "        'start_time',\n",
    "        'wind_dir_sin', 'wind_dir_cos',\n",
    "        'wind_speed_kt', 'is_wind_variable',\n",
    "        'vis_ge_10km', 'vis_category',\n",
    "        'temperature_c', 'spread_c', 'qnh_hpa'\n",
    "    ]],\n",
    "    on='start_time',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2a39f5",
   "metadata": {},
   "source": [
    "Ferien extrahieren (National + ZH spezifisch) und zu helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e502000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import holidays\n",
    "\n",
    "helper_df['start_time_local'] = helper_df['start_time'].dt.tz_convert('Europe/Zurich')\n",
    "\n",
    "# 2) Local date in Zurich -> Nötig, da Ferien jeweils in LT angegeben sind und nicht in UTC\n",
    "helper_df['start_time_local'] = helper_df['start_time'].dt.tz_convert('Europe/Zurich') \n",
    "helper_df['date_local'] = helper_df['start_time_local'].dt.date\n",
    "\n",
    "# 3) Build Zurich holidays for the years present\n",
    "years_needed = sorted(helper_df['start_time_local'].dt.year.unique().tolist())\n",
    "ch_holidays_zh = holidays.Switzerland(prov='ZH', years=years_needed)\n",
    "\n",
    "# 4) Use a SET of dates for membership checks (critical!)\n",
    "holiday_dates = set(ch_holidays_zh.keys())\n",
    "\n",
    "helper_df['is_holiday'] = helper_df['date_local'].isin(holiday_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12993a0c",
   "metadata": {},
   "source": [
    "Ferien zum features_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c62a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.merge(\n",
    "    helper_df[['start_time', 'is_holiday']],\n",
    "    on='start_time',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ca3824",
   "metadata": {},
   "source": [
    "Anzahl Scheduled Departures & Arrivals für 30' Intervalle aufsummieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca3b620",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scheduled departures per 30' interval\n",
    "DEP_TIME_COL = \"scheduled_off_block_time_utc\"     \n",
    "dep_sched = (\n",
    "    dep.assign(slot_start = dep[DEP_TIME_COL].dt.floor(\"30min\"))\n",
    "       .groupby(\"slot_start\", as_index=False)\n",
    "       .size()\n",
    "       .rename(columns={\"size\": \"scheduled_departures\"})\n",
    ")\n",
    "\n",
    "# b) Scheduled arrivals per 30' interval\n",
    "ARR_TIME_COL = \"scheduled_in_block_time_utc\"      \n",
    "arr_sched = (\n",
    "    arr.assign(slot_start = arr[ARR_TIME_COL].dt.floor(\"30min\"))\n",
    "       .groupby(\"slot_start\", as_index=False)\n",
    "       .size()\n",
    "       .rename(columns={\"size\": \"scheduled_arrivals\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f887af3",
   "metadata": {},
   "source": [
    "Anzahl Scheduled Departures & Arrivals zum helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2598e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merge departures into helper_df ---\n",
    "helper_df = (\n",
    "    helper_df\n",
    "    .merge(dep_sched, left_on=\"start_time\", right_on=\"slot_start\", how=\"left\")\n",
    "    .drop(columns=[\"slot_start\"])\n",
    ")\n",
    "\n",
    "# --- Merge arrivals into helper_df ---\n",
    "helper_df = (\n",
    "    helper_df\n",
    "    .merge(arr_sched, left_on=\"start_time\", right_on=\"slot_start\", how=\"left\")\n",
    "    .drop(columns=[\"slot_start\"])\n",
    ")\n",
    "\n",
    "# --- Fill missing values (slots with no traffic) ---\n",
    "helper_df[[\"scheduled_departures\", \"scheduled_arrivals\"]] = (\n",
    "    helper_df[[\"scheduled_departures\", \"scheduled_arrivals\"]]\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a93e3",
   "metadata": {},
   "source": [
    "Anzahl Scheduled Departures & Arrivals zum Features df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abad070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring just the relevant columns into features_df\n",
    "features_df = features_df.merge(\n",
    "    helper_df[[\"start_time\", \"scheduled_departures\", \"scheduled_arrivals\"]],\n",
    "    on=\"start_time\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Optional safety: fill NaNs (in case of unmatched times)\n",
    "features_df[[\"scheduled_departures\", \"scheduled_arrivals\"]] = (\n",
    "    features_df[[\"scheduled_departures\", \"scheduled_arrivals\"]]\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a7132",
   "metadata": {},
   "source": [
    "Departure Delay berechnen -> Annahme: Departure Delay = Differenz zw. actual und scheduled off block time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ffc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dep['delay_departure_minutes'] = (dep['actual_off_block_time_utc'] - dep['scheduled_off_block_time_utc']).dt.total_seconds() / 60.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecec227",
   "metadata": {},
   "source": [
    "Average Departure Delay berechnen pro 30' Intervall berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef1b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEP_ACT_TIME_COL = \"scheduled_off_block_time_utc\"   \n",
    "DELAY_COL        = \"delay_departure_minutes\"        \n",
    "\n",
    "# 1) Compute per-slot average actual departure delay (for the slot itself)\n",
    "dep_delay_slot = (\n",
    "    dep.assign(slot_start = dep[DEP_ACT_TIME_COL].dt.floor(\"30min\"))\n",
    "       .groupby(\"slot_start\", as_index=False)[DELAY_COL]\n",
    "       .mean()\n",
    "       .rename(columns={DELAY_COL: \"avg_dep_delay_slot_minutes\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a32dd4",
   "metadata": {},
   "source": [
    "Average Departure Delay zu helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898bf8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure time order for safe merging\n",
    "helper_df = helper_df.sort_values(\"start_time\").copy()\n",
    "dep_delay_slot = dep_delay_slot.sort_values(\"slot_start\").copy()\n",
    "\n",
    "# Add avg delay of the *current* slot into helper_df\n",
    "helper_df = (\n",
    "    helper_df\n",
    "    .merge(dep_delay_slot, left_on=\"start_time\", right_on=\"slot_start\", how=\"left\")\n",
    "    .drop(columns=[\"slot_start\"])\n",
    ")\n",
    "\n",
    "# Fill slots without departures with 0\n",
    "helper_df[\"avg_dep_delay_slot_minutes\"] = helper_df[\"avg_dep_delay_slot_minutes\"].fillna(0).astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a6d687",
   "metadata": {},
   "source": [
    "Average Departure Delay vom vorherigem Slot und von den letzten 6 Stunden zu helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4380802a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort again to be explicit\n",
    "helper_df = helper_df.sort_values(\"start_time\").copy()\n",
    "\n",
    "# Previous slot average (lag 1)\n",
    "helper_df[\"avg_dep_delay_prev_slot_minutes\"] = (\n",
    "    helper_df[\"avg_dep_delay_slot_minutes\"].shift(1).fillna(0)\n",
    ")\n",
    "\n",
    "# Rolling average over the past 6h (12 slots) — exclude current slot via shift\n",
    "helper_df[\"avg_dep_delay_past_6h_minutes\"] = (\n",
    "    helper_df[\"avg_dep_delay_slot_minutes\"]\n",
    "    .shift(1)                           # exclude current slot\n",
    "    .rolling(window=12, min_periods=1)  # 12 * 30min = 6h\n",
    "    .mean()\n",
    "    .fillna(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facd50a5",
   "metadata": {},
   "source": [
    "Average Departure Delay vom aktuellen und vorherigem Slot + Average Departure Delay der letzten 6 Stunden zum features_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f568800",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_bring = [\n",
    "    \"start_time\",\n",
    "    \"avg_dep_delay_slot_minutes\",\n",
    "    \"avg_dep_delay_prev_slot_minutes\",\n",
    "    \"avg_dep_delay_past_6h_minutes\",\n",
    "]\n",
    "\n",
    "features_df = features_df.merge(\n",
    "    helper_df[cols_to_bring],\n",
    "    on=\"start_time\",\n",
    "    how=\"left\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b37dd5",
   "metadata": {},
   "source": [
    "Anzahl Scheduled Departures pro Flugzeugkategorie (B-F) pro 30' Slot extrahieren und zu helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7805035c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define key columns ---\n",
    "DEP_ACT_TIME_COL = \"scheduled_off_block_time_utc\"\n",
    "CODE_COL         = \"aircraft_category_icao_code\"\n",
    "\n",
    "# 1) Assign departures to 30-minute slots and normalize code letters\n",
    "dep_slots = (\n",
    "    dep.assign(\n",
    "        slot_start  = dep[DEP_ACT_TIME_COL].dt.floor(\"30min\"),\n",
    "        code_letter = dep[CODE_COL].astype(str).str.upper().str.strip()\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2) Keep valid ICAO letters (B–F)\n",
    "valid_letters = list(\"BCDEF\")\n",
    "dep_slots = dep_slots[dep_slots[\"code_letter\"].isin(valid_letters)]\n",
    "\n",
    "# 3) Count departures per slot and aircraft category\n",
    "counts_long = (\n",
    "    dep_slots.groupby([\"slot_start\", \"code_letter\"], as_index=False)\n",
    "             .size()\n",
    "             .rename(columns={\"size\": \"count\"})\n",
    ")\n",
    "\n",
    "# 4) Pivot: scheduled_B … scheduled_F\n",
    "counts_wide = (\n",
    "    counts_long.pivot(index=\"slot_start\", columns=\"code_letter\", values=\"count\")\n",
    "               .reindex(columns=valid_letters, fill_value=0)\n",
    "               .add_prefix(\"scheduled_\")\n",
    "               .reset_index()\n",
    ")\n",
    "\n",
    "# 5) Merge into helper_df\n",
    "helper_df = (\n",
    "    helper_df.merge(counts_wide, left_on=\"start_time\", right_on=\"slot_start\", how=\"left\")\n",
    "             .drop(columns=[\"slot_start\"])\n",
    ")\n",
    "\n",
    "# 6) Replace NaN (slots with no departures) with 0\n",
    "scheduled_cols = [f\"scheduled_{c}\" for c in valid_letters]\n",
    "helper_df[scheduled_cols] = helper_df[scheduled_cols].fillna(0).astype(\"int64\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85614f",
   "metadata": {},
   "source": [
    "Anzahl Scheduled Departures pro Flugzeugkategorie (B-F) zu features_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc14e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to bring over from helper_df\n",
    "valid_letters  = list(\"BCDEF\")\n",
    "scheduled_cols = [f\"scheduled_{c}\" for c in valid_letters]\n",
    "\n",
    "# Merge into features_df via start_time\n",
    "features_df = features_df.merge(\n",
    "    helper_df[['start_time'] + scheduled_cols],\n",
    "    on='start_time',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Ensure zeros for slots with no departures and stable dtype\n",
    "features_df[scheduled_cols] = features_df[scheduled_cols].fillna(0).astype('int64')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1caba9d",
   "metadata": {},
   "source": [
    "Überprüfen, welche RWY Concepts im Dataset überhaupt existieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13143b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(dep['runway_concept'].dropna().unique().tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49f72a",
   "metadata": {},
   "source": [
    "Zählen, wie oft welches RWY Concept im Dataset gebraucht wurde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372afa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your feature column is called 'scheduled_rwy_concept'\n",
    "concept_counts = dep['runway_concept'].value_counts().sort_values(ascending=False)\n",
    "(dep[\"runway_concept\"]\n",
    " .value_counts(normalize=True)\n",
    " .mul(100)\n",
    " .round(5)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8ffbd1",
   "metadata": {},
   "source": [
    "Actual und Scheduled Runway Concept extrahieren und zu helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Das Runway Concept ist im Dataset pro Flug angegeben.\n",
    "Hier wird das häufigste Runway Concept pro 30' Slot extrahiert und zu helper_df hinzugefügt.\n",
    "Scheduled Concept ist der Wert des vorherigen Slots. \n",
    "Ist kein Konzept hinterlegt, wird NaN gesetzt.\n",
    "Da einige Concepts <0.1% des gesamten Datasets ausmachen, werden diese als neues Konzept \"other_concept\" zusammengefasst\n",
    "'''\n",
    "\n",
    "rwy_slot = (\n",
    "    dep.assign(slot_start = dep['scheduled_off_block_time_utc'].dt.floor('30min'))\n",
    "       .groupby('slot_start', as_index=False)['runway_concept']\n",
    "       .agg(lambda x: x.mode().iat[0] if not x.mode().empty else np.nan)\n",
    "       .rename(columns={'runway_concept': 'actual_rwy_concept'})\n",
    ")\n",
    "\n",
    "helper_df = (\n",
    "    helper_df.merge(rwy_slot, left_on='start_time', right_on='slot_start', how='left')\n",
    "             .drop(columns=['slot_start'])\n",
    ")\n",
    "\n",
    "# Make it categorical (important for XGBoost)\n",
    "helper_df['actual_rwy_concept'] = helper_df['actual_rwy_concept'].astype('category')\n",
    "\n",
    "helper_df = helper_df.sort_values('start_time').copy()\n",
    "helper_df['scheduled_rwy_concept'] = helper_df['actual_rwy_concept'].shift(1)\n",
    "helper_df['scheduled_rwy_concept'] = helper_df['scheduled_rwy_concept'].astype('category')\n",
    "\n",
    "#Seltene Konzepte (<1%) finden\n",
    "freq = helper_df['actual_rwy_concept'].value_counts(normalize=True, dropna=True)\n",
    "rare_cats = freq[freq < 0.01].index  # <1 %\n",
    "\n",
    "# 5) Seltene Konzepte zu \"other_concept\" mappen (für beide Spalten konsistent)\n",
    "for col in ['actual_rwy_concept', 'scheduled_rwy_concept']:\n",
    "    # Kategorie \"other_concept\" hinzufügen\n",
    "    helper_df[col] = helper_df[col].cat.add_categories(['other_concept'])\n",
    "    # seltene Kategorien ersetzen\n",
    "    helper_df.loc[helper_df[col].isin(rare_cats), col] = 'other_concept'\n",
    "    # ungenutzte Kategorien aufräumen\n",
    "    helper_df[col] = helper_df[col].cat.remove_unused_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52ff0ec",
   "metadata": {},
   "source": [
    "Scheduled Runway Concept zu features_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c7295",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.merge(\n",
    "    helper_df[['start_time', 'scheduled_rwy_concept']],\n",
    "    on='start_time',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd30a1f5",
   "metadata": {},
   "source": [
    "Verhältnis von Departures und Arrivals ausrechnen und zu helper_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e3f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_df[\"arr_dep_ratio\"] = (\n",
    "    helper_df[\"scheduled_arrivals\"]\n",
    "    / helper_df[\"scheduled_departures\"].replace(0, np.nan)\n",
    ")\n",
    "\n",
    "# Replace divisions by 0 and NaNs (no traffic) with 0\n",
    "helper_df[\"arr_dep_ratio\"] = helper_df[\"arr_dep_ratio\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6490374",
   "metadata": {},
   "source": [
    "Verhältnis von Departures und Arrivals zu features_df hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fa2714",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df.merge(\n",
    "    helper_df[[\"start_time\", \"arr_dep_ratio\"]],\n",
    "    on=\"start_time\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "features_df[\"arr_dep_ratio\"] = features_df[\"arr_dep_ratio\"].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809dc770",
   "metadata": {},
   "outputs": [],
   "source": [
    "helper_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed82bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1c7484",
   "metadata": {},
   "source": [
    "features_df und helper_df als Pickle-File exportieren in den Ordner data/processed. Anders als bei CSV gehen hier Datatypes nicht verloren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8f8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_f = \"../data/processed/features.pkl\"\n",
    "output_path_h = \"../data/processed/helper.pkl\"\n",
    "\n",
    "#Save cleaned dataframe to pkl\n",
    "features_df.to_pickle(output_path_f)\n",
    "helper_df.to_pickle(output_path_h)\n",
    "\n",
    "print(f\" Cleaned file saved to: {output_path_f}\")\n",
    "print(f\" Cleaned file saved to: {output_path_h}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
